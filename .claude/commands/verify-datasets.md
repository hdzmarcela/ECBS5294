---
description: Comprehensive verification of course datasets for pedagogical soundness and technical correctness
---

# Dataset Verification Command

You are verifying datasets for **ECBS5294: Introduction to Data Science: Working with Data**.

**CRITICAL PRINCIPLE:** This verification must be **ADAPTIVE** and **GOAL-ORIENTED**, not hardcoded to specific implementation details.

Focus on: **"Can this dataset achieve the syllabus learning objectives?"**
Not: **"Does this match my expectations of what Day 2 should look like?"**

---

## Command Usage

The user will invoke this command with:
- `/verify-datasets` - Verify all days
- `/verify-datasets 1` - Verify only Day 1
- `/verify-datasets 2` - Verify only Day 2
- `/verify-datasets 3` - Verify only Day 3

---

## Your Verification Process

### Phase 1: Parse Learning Objectives (REQUIRED FIRST STEP)

**Read `syllabus.md`** and extract learning objectives for the requested day(s).

**Day 1 Block A - Expected Learning:**
- Tidy data principles
- UID/Primary key identification
- Type handling (dates, floats, booleans)
- Missing values management
- Transforming messy ‚Üí tidy

**Day 1 Block B - Expected Learning:**
- Single-table SQL (SELECT, WHERE, ORDER BY, GROUP BY/HAVING)
- NULL behavior
- Window functions: ROW_NUMBER(), LAG(), moving averages
- Understanding windows preserve rows vs GROUP BY collapses

**Day 2 Block A - Expected Learning:**
- INNER/LEFT/RIGHT/FULL JOINs
- Primary key / Foreign key relationships
- ERD understanding
- Join patterns (anti-join, semi-join)
- Duplicate inflation awareness
- Grouping after joins

**Day 2 Block B - Expected Learning:**
- JSON parsing (file or endpoint)
- Nested structure normalization (dict/list)
- Flattening nested objects
- Exploding nested arrays
- Persisting to DuckDB
- Basic typing (dates, etc.)

**Day 3 Block A - Expected Learning:**
- Data quality issues (CSV traps, date handling, categoricals)
- Pipeline patterns: bronze ‚Üí silver ‚Üí gold
- Idempotent transforms
- Validations as code (PK unique, required non-nulls, date windows)

**Day 3 Block B:**
- In-class exam (no dataset needed)

---

### Phase 2: File System Checks (Universal)

Run these checks for data/day{N}/ directory:

```
‚úÖ Directory exists
‚úÖ README.md exists and has substantial content (min 100 lines)
‚úÖ Expected files present (inferred from README)
‚úÖ No files > 100MB (git hard limit)
‚ö†Ô∏è  Flag files > 50MB (git warning threshold)
‚úÖ No unencrypted solution files (*_solution.ipynb, *_solution.py)
‚úÖ All documented files actually exist
‚úÖ All files tracked in git
```

---

### Phase 3: Documentation Completeness (Universal)

Check README.md contains:

```
‚úÖ Dataset source and URL
‚úÖ License information explicitly stated
‚úÖ License allows educational use (verify for known licenses)
‚úÖ Attribution with proper format (as required by license)
‚úÖ Dataset description (what it represents)
‚úÖ Column/field documentation (schema defined)
‚úÖ Sample code/queries (at least 2 examples)
‚úÖ Learning objectives explicitly listed
‚úÖ Connection to syllabus topics mentioned
‚úÖ "Intentional data characteristics" or similar section
‚úÖ Code blocks properly formatted (```python, ```sql)
‚úÖ File paths are relative (no /Users/, no C:\, no absolute paths)
```

---

### Phase 4: Goal-Oriented Pedagogical Checks (ADAPTIVE!)

**This is the most important phase. You must check if the dataset ACHIEVES the learning objectives, not if it matches a specific implementation.**

#### For Day 1 Block A (Tidy Data Teaching)

**Objective from syllabus:** "Load a messy CSV ‚Üí tidy ‚Üí designate UID ‚Üí one summary table"

**Adaptive checks:**
```python
‚úÖ Dataset has documented "messiness" (README mentions data quality issues)
‚úÖ Types of messiness documented (what students encounter)
‚úÖ "What clean looks like" is defined (success criteria stated)
‚úÖ Primary key is identifiable or can be constructed
‚úÖ At least ONE tidy principle demonstrable:
   - Missing values (multiple representations)
   - Type inconsistencies
   - Structural issues
   - Duplicate rows
   - Unnormalized structure

Assessment: ‚úÖ PASS if cleaning practice is possible and success criteria defined
           ‚ùå FAIL if dataset is already tidy with no teaching value
           ‚ùå FAIL if "clean" criteria not documented
```

#### For Day 1 Block B (SQL + Window Functions)

**Objective from syllabus:** "Single-table SQL + window functions (ROW_NUMBER, LAG, moving avg)"

**Adaptive checks:**
```python
‚úÖ Dataset has temporal/sequential dimension (dates, timestamps, ordering)
‚úÖ Dataset has groups/IDs (for PARTITION BY teaching)
‚úÖ Row count > 100 (meaningful for aggregation)
‚úÖ Sample queries in README demonstrate:
   - Basic SELECT/WHERE/GROUP BY
   - At least ONE window function example
‚úÖ Data loads in DuckDB without errors
‚úÖ Can demonstrate "windows preserve rows" vs "GROUP BY collapses"

Assessment: ‚úÖ PASS if window functions are demonstrable with this data
           ‚ùå FAIL if no temporal/sequential dimension exists
           ‚ùå FAIL if dataset too small for meaningful aggregation
```

#### For Day 2 Block A (JOIN Teaching)

**Objective from syllabus:** "INNER/LEFT/RIGHT/FULL joins; PK/FK; join patterns"

**Adaptive checks (IMPORTANT - Be Flexible!):**
```python
‚úÖ Multiple related tables exist (at least 2, preferably 3+)
‚úÖ PK/FK relationships documented in README
‚úÖ Can demonstrate INNER JOIN?
   ‚Üí Check: Matching rows exist between tables

‚úÖ Can demonstrate LEFT JOIN?
   ‚Üí Check: Unmatched rows exist (rows in left without match in right)
   ‚Üí OR: README documents alternative teaching approach

‚úÖ Can demonstrate RIGHT JOIN?
   ‚Üí Check: Unmatched rows exist (rows in right without match in left)
   ‚Üí OR: README documents "taught conceptually" with explanation
   ‚Üí NOTE: RIGHT JOIN is rarely used in industry, conceptual teaching is acceptable

‚úÖ Can demonstrate FULL OUTER JOIN?
   ‚Üí Check: Unmatched rows on both sides OR combination approach

‚úÖ One-to-many relationship exists (for row inflation teaching)
‚úÖ README shows sample JOIN queries
‚úÖ ER diagram or relationship description present

Assessment: ‚úÖ PASS if all four join types achievable (directly OR conceptually)
           ‚ö†Ô∏è  WARN if some joins only conceptual but approach is documented
           ‚ùå FAIL if cannot teach required joins AND no documented strategy
           ‚ùå FAIL if no PK/FK relationships documented
```

**CRITICAL FOR RIGHT JOIN:** If dataset has no unmatched rows for RIGHT JOIN:
- Check README for documented teaching strategy
- Check for note about industry practices (RIGHT JOIN rarely used)
- If documented, this is **pedagogically acceptable**

#### For Day 2 Block B (JSON Normalization)

**Objective from syllabus:** "JSON ‚Üí dict/list; normalization; persist to DuckDB"

**Adaptive checks:**
```python
‚úÖ JSON file exists and parses without errors
‚úÖ Has nested structure - at least ONE of:
   - Nested object (dict within dict) for flattening
   - Nested array of objects for exploding (one-to-many)
   - Array of simple values for bridge table (many-to-many)
‚úÖ Normalization opportunities documented
‚úÖ Sample code shows:
   - JSON loading
   - Structure inspection
   - Flattening OR table creation
   - DuckDB persistence
‚úÖ At least one one-to-many relationship demonstrable
‚úÖ Primary key generation shown (if needed for child tables)

Assessment: ‚úÖ PASS if JSON requires normalization to become relational
           ‚ùå FAIL if JSON is already flat (no teaching value)
           ‚ùå FAIL if no nested structures present
```

#### For Day 3 Block A (Pipeline Teaching)

**Objective from syllabus:** "Bronze ‚Üí silver ‚Üí gold; validations as code"

**Adaptive checks:**
```python
‚úÖ Multiple file formats OR multiple sources
‚úÖ Data quality issues present (for validation teaching)
‚úÖ README documents pipeline stages
‚úÖ Validation examples shown (PK unique, non-nulls, date range, etc.)
‚úÖ Sample code demonstrates idempotent transforms
‚úÖ Bronze/Silver/Gold pattern explained

Assessment: ‚úÖ PASS if multi-stage pipeline is demonstrable
           ‚ùå FAIL if only single clean file (no pipeline needed)
```

---

### Phase 5: Technical Verification (Universal)

**Load and validate all datasets:**

**For CSV files:**
```python
For each CSV in data/day{N}/:
  ‚úÖ Loads with pandas without errors
  ‚úÖ Loads with DuckDB without errors
  ‚úÖ Row count within ¬±10% of documented count (if documented)
  ‚úÖ Column names match documentation
  ‚úÖ No completely empty files (0 rows)
  ‚úÖ Primary keys unique (if documented as PK)
  ‚úÖ No entirely NULL columns (unless documented)
```

**For JSON files:**
```python
For each JSON in data/day{N}/:
  ‚úÖ Parses without JSON syntax errors
  ‚úÖ Top-level structure matches documentation
  ‚úÖ Nested structures present (if documented)
  ‚úÖ Row/record count within ¬±10% of documented
```

**For Parquet files:**
```python
For each Parquet in data/day{N}/:
  ‚úÖ Loads with pyarrow or DuckDB
  ‚úÖ Schema readable
  ‚úÖ Row count reasonable
```

---

### Phase 6: Code Sample Validation

**Extract all code blocks from README and test:**

**For SQL code blocks:**
```python
For each SQL query in README:
  1. Extract the query
  2. Replace table references with actual file paths (relative)
  3. Run in DuckDB
  4. Verify:
     ‚úÖ Query runs without errors
     ‚úÖ Query returns results (unless documented as showing empty case)
     ‚úÖ No absolute paths in query
```

**For Python code blocks:**
```python
For each Python script in README:
  1. Extract the code
  2. Run in isolated environment
  3. Verify:
     ‚úÖ Code runs without errors
     ‚úÖ Imports are standard (pandas, json, duckdb, requests)
     ‚úÖ No absolute paths
     ‚úÖ Relative paths resolve correctly
```

---

### Phase 7: Cross-Reference Validation

**Verify documentation matches reality:**

```python
‚úÖ Documented row counts match actual (¬±10% tolerance)
‚úÖ Documented column names match actual schema
‚úÖ Example data values in README actually exist in dataset
‚úÖ Sample queries reference actual table/column names
‚úÖ Learning objectives in README align with syllabus objectives
‚úÖ File paths in documentation point to existing files
```

---

## Output Format

**Generate a comprehensive report using this structure:**

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
[‚úÖ or ‚ùå] Day {N} Dataset Verification: [PASSED or ISSUES FOUND]
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìö Syllabus Learning Objectives (Day {N})
  Block A: [objectives from syllabus]
  Block B: [objectives from syllabus]

üìÅ File System ({X}/{Y} checks)
  [List each check with ‚úÖ/‚ùå/‚ö†Ô∏è]
  [If failures, explain what's missing]

üìñ Documentation ({X}/{Y} checks)
  [List each check with ‚úÖ/‚ùå/‚ö†Ô∏è]
  [Note any missing sections]

üéì Pedagogical Soundness - Block A
  [For each learning objective:]
  [Check if dataset achieves it]
  [If not directly, check if alternative approach documented]

  Assessment: [ACHIEVES or CANNOT ACHIEVE] syllabus objectives
  [Explanation of decision]

üéì Pedagogical Soundness - Block B
  [Same structure as Block A]

üî¨ Technical Verification ({X}/{Y} checks)
  [List each technical check]
  [Note any loading errors or mismatches]

üíª Code Samples ({X}/{Y} tested)
  [Report on each code block tested]
  [Note any failures with error messages]

üìä Data Quality ({X}/{Y} checks)
  [Cross-reference checks]
  [Note any inconsistencies]

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
RESULT: {X}/{Y} checks passed ({Z} warnings, {W} failures)

[‚úÖ or ‚ùå] VERDICT: Dataset is [or is NOT] pedagogically sound and ready for teaching

[If failures:]
CRITICAL ISSUES TO FIX:
1. [Issue with explanation of impact on teaching]
2. [Issue with suggestion for fix]
...

[If warnings:]
NOTES:
- [Explanation of each warning]
- [Why it's acceptable or should be addressed]

Next steps: [What should be done, if anything]
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```

---

## Key Reminders

1. **Be Adaptive:** Different datasets for same objective should both pass
2. **Check Goals, Not Implementation:** Focus on "can we teach this?" not "does it match exactly?"
3. **Accept Documented Alternatives:** If README explains pedagogical approach, that counts
4. **Industry Context Matters:** Some patterns (like RIGHT JOIN) being rare is pedagogically relevant
5. **Be Thorough:** Check everything systematically
6. **Be Clear:** Explain WHY something passes or fails
7. **Link to Syllabus:** Every failure should reference what objective can't be achieved

---

## Special Cases

**RIGHT JOIN Teaching:**
- If no unmatched rows for RIGHT JOIN examples
- Check if README documents "taught conceptually"
- Check if README mentions industry practice (rarely used)
- If well-documented, this is ‚úÖ PASS with ‚ö†Ô∏è NOTE

**Messy Data:**
- For tidy data teaching, messiness is REQUIRED
- But must be documented with "what clean looks like"

**Large Files:**
- Files 50-100MB: ‚ö†Ô∏è WARN (GitHub will warn but allow)
- Files > 100MB: ‚ùå FAIL (GitHub hard limit)

**Missing Sections:**
- If section is in syllabus but not in README: ‚ùå FAIL
- If section is pedagogically important: ‚ùå FAIL
- If section is minor: ‚ö†Ô∏è WARN

---

Now run the verification for the requested day(s) and generate the comprehensive report.
