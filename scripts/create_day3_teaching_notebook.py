#!/usr/bin/env python3
"""
Create Day 3 Block A teaching notebook:
- Pipelines (bronze/silver/gold)
- Validations as code
- Data survival tips
- Work habits

Following established pedagogical patterns from Day 1/2
"""

import json
from pathlib import Path

# Notebook cells
cells = []
cell_counter = 0

def add_markdown_cell(content):
    """Add a markdown cell to the notebook"""
    global cell_counter
    cell_id = f"cell-{cell_counter}"
    cell_counter += 1

    cells.append({
        "cell_type": "markdown",
        "id": cell_id,
        "metadata": {},
        "source": [line + "\n" for line in content.split("\n")]
    })

def add_code_cell(content):
    """Add a code cell to the notebook"""
    global cell_counter
    cell_id = f"cell-{cell_counter}"
    cell_counter += 1

    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "id": cell_id,
        "metadata": {},
        "outputs": [],
        "source": [line + "\n" for line in content.split("\n")]
    })

# ============================================================================
# HEADER & LEARNING OBJECTIVES
# ============================================================================

add_markdown_cell("""# Day 3, Block A: Data Pipelines & Real-World Validation

**Duration:** 100 minutes (13:30‚Äì15:10)
**Course:** ECBS5294 - Introduction to Data Science: Working with Data
**Instructor:** Eduardo Ari√±o de la Rubia

---

## Learning Objectives

By the end of this session, you will be able to:

1. Explain the **bronze ‚Üí silver ‚Üí gold** pipeline pattern and why it matters
2. Design idempotent data transformations
3. Write **assertions** to validate data quality programmatically
4. Identify and handle common real-world data problems (dates, types, nulls)
5. Apply the **pipeline pattern** to a real dataset
6. Communicate data quality issues and limitations to stakeholders

---""")

# ============================================================================
# SECTION 1: WHY PIPELINES?
# ============================================================================

add_markdown_cell("""## 1. Why Data Pipelines?

### The Problem: One-Off Analysis Doesn't Scale

Picture this scenario:

> **Monday morning:** Your manager asks for sales analysis. You load CSV, clean it, make charts. Email results.
>
> **Tuesday:** "Can you refresh those numbers with today's data?"
> You: "Uh... let me re-run everything..."
>
> **Wednesday:** "Actually, can you also include last month?"
> You: "I need to start over... my notebook is a mess..."
>
> **Thursday:** "Why don't your numbers match Finance?"
> You: "I... I'm not sure which cleaning steps I applied..."

**You've hit the wall.** Ad-hoc analysis breaks down when:
- Data updates regularly
- Multiple people need consistent results
- Stakeholders ask "how did you get this number?"
- Requirements change
- You need to reproduce results months later

**The solution?** A systematic, repeatable pipeline.

---

### The Business Case for Pipelines

Pipelines aren't just "good engineering"‚Äîthey're **strategic assets**:

| Without Pipelines | With Pipelines |
|-------------------|----------------|
| ‚ùå Analyst spends 2 hours manually refreshing reports | ‚úÖ Automated refresh, analyst reviews in 10 minutes |
| ‚ùå "I don't remember how I cleaned this" | ‚úÖ Every step documented in code |
| ‚ùå Numbers differ between teams | ‚úÖ Single source of truth |
| ‚ùå Errors discovered weeks later | ‚úÖ Validations catch problems immediately |
| ‚ùå New analyst takes weeks to understand | ‚úÖ Pipeline is self-documenting |

**ROI:** A day spent building a pipeline saves weeks of manual work and prevents costly errors.

---

### Real-World Example: The $2M Mistake

**Company:** Online retailer
**Problem:** Revenue dashboard showed consistent growth
**Reality:** Data pipeline had a bug that duplicated 15% of transactions

**Impact:**
- üî• **$2M overstated revenue** in quarterly earnings call
- üî• SEC investigation
- üî• CFO resigned
- üî• Stock price dropped 25%

**Root cause:** No validation step to check for duplicate order IDs

**Lesson:** Validations aren't optional. They're insurance against career-ending mistakes.

---""")

# ============================================================================
# SECTION 2: THE BRONZE-SILVER-GOLD PATTERN
# ============================================================================

add_markdown_cell("""## 2. The Bronze-Silver-Gold Pattern

### The Three Stages

> **"Preserve the original, clean incrementally, aggregate deliberately."**

This pattern comes from data engineering (Databricks "medallion architecture"), but applies to any data work.

#### **Bronze Layer: Raw Ingestion**
- **Purpose:** Preserve original data exactly as received
- **No transformations** (except maybe decompression, format conversion)
- **Keep everything:** Even if it looks wrong
- **Why:** Reproducibility, debugging, legal/compliance

**Example:** Raw CSV as downloaded, JSON from API

#### **Silver Layer: Clean & Validated**
- **Purpose:** Analysis-ready data
- **Fix types:** Strings ‚Üí dates, numbers ‚Üí floats
- **Handle nulls:** Standardize representations
- **Validate:** Primary keys, foreign keys, business rules
- **Document:** What was fixed and why

**Example:** Typed tables in DuckDB with assertions

#### **Gold Layer: Business Metrics**
- **Purpose:** Aggregated, joined, ready for reporting
- **Joins:** Combine related tables
- **Aggregations:** Summaries, KPIs, metrics
- **Denormalized:** Optimized for specific analyses

**Example:** Dashboard tables, summary metrics, report views

---

### Why Three Layers?

**Can't I just clean and analyze in one step?**

You can, but:
- ‚ùå If you discover a cleaning mistake, start over from scratch
- ‚ùå Can't debug: "Was the data bad, or my logic?"
- ‚ùå Can't reuse cleaned data for different analyses
- ‚ùå Hard for others to understand your process

**With layers:**
- ‚úÖ Fix a cleaning bug ‚Üí just re-run Silver forward
- ‚úÖ Try different aggregation ‚Üí just re-run Gold
- ‚úÖ Debug: Check Bronze (source issue?) vs Silver (cleaning issue?) vs Gold (logic issue?)
- ‚úÖ Multiple Gold tables from same Silver source
- ‚úÖ Clear separation of concerns

---

### Visual: Pipeline Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   BRONZE    ‚îÇ  Raw data as received
‚îÇ  (orders.   ‚îÇ  - Preserve original
‚îÇ   csv)      ‚îÇ  - No transformations
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì  Clean & validate
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SILVER    ‚îÇ  Analysis-ready
‚îÇ  (orders_   ‚îÇ  - Typed columns
‚îÇ   clean)    ‚îÇ  - Nulls handled
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - Assertions passed
       ‚îÇ
       ‚Üì  Join & aggregate
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    GOLD     ‚îÇ  Business metrics
‚îÇ  (daily_    ‚îÇ  - KPIs calculated
‚îÇ   sales)    ‚îÇ  - Joined tables
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - Ready for reporting
```

---""")

# ============================================================================
# SECTION 3: EXAMPLE PIPELINE
# ============================================================================

add_markdown_cell("""## 3. Building a Pipeline: Example

We'll build a three-stage pipeline using Olist order data.

**Scenario:** You're an analyst at an e-commerce company. Every morning, you need to report yesterday's sales metrics. Let's build a pipeline that can run daily without breaking.

---

### Step 0: Setup""")

add_code_cell("""# Setup
import pandas as pd
import duckdb
import numpy as np
from IPython.display import display

# Suppress warnings for cleaner teaching output
import warnings
warnings.filterwarnings('ignore')

# Connect to DuckDB
con = duckdb.connect(':memory:')

print("‚úÖ Setup complete")""")

add_markdown_cell("""---

### Bronze Layer: Raw Ingestion

**Goal:** Load data exactly as received. No cleaning, just load.""")

add_code_cell("""# BRONZE: Load raw data
print("=== BRONZE LAYER: Raw Ingestion ===\\n")

# Load orders as-is
bronze_orders = con.execute("""\"
    CREATE TABLE bronze_orders AS
    SELECT * FROM 'data/day3/teaching/olist_orders_subset.csv'
\"\""").df()

bronze_customers = con.execute("""\"
    CREATE TABLE bronze_customers AS
    SELECT * FROM 'data/day3/teaching/olist_customers_subset.csv'
\"\""").df()

bronze_items = con.execute("""\"
    CREATE TABLE bronze_order_items AS
    SELECT * FROM 'data/day3/teaching/olist_order_items_subset.csv'
\"\""").df()

# Check what we loaded
print("Loaded bronze tables:")
print(f"  - bronze_orders: {con.execute('SELECT COUNT(*) FROM bronze_orders').fetchone()[0]} rows")
print(f"  - bronze_customers: {con.execute('SELECT COUNT(*) FROM bronze_customers').fetchone()[0]} rows")
print(f"  - bronze_order_items: {con.execute('SELECT COUNT(*) FROM bronze_order_items').fetchone()[0]} rows")

print("\\n‚úÖ Bronze layer complete: Raw data preserved")""")

add_markdown_cell("""**What we did:**
- Loaded CSVs directly into DuckDB
- No transformations
- No validation (yet)
- Just: "Here's what we received"

**Why keep bronze?**
- If we discover cleaning mistakes later, we can start over
- Audit trail: "What did the source system give us?"
- Legal/compliance: Original data preserved

---

### Silver Layer: Clean & Validate

**Goal:** Transform into analysis-ready format with validation.""")

add_code_cell("""# SILVER: Clean and validate
print("=== SILVER LAYER: Clean & Validate ===\\n")

# Transform orders: Fix types, validate
con.execute("""\"
    CREATE TABLE silver_orders AS
    SELECT
        order_id,
        customer_id,
        order_status,
        TRY_CAST(order_purchase_timestamp AS TIMESTAMP) as order_date,
        TRY_CAST(order_delivered_customer_date AS TIMESTAMP) as delivery_date
    FROM bronze_orders
    WHERE order_id IS NOT NULL  -- Remove any rows without ID
\"\""")

# Transform customers: Clean types
con.execute("""\"
    CREATE TABLE silver_customers AS
    SELECT
        customer_id,
        customer_zip_code_prefix as zip_code,
        customer_city as city,
        customer_state as state
    FROM bronze_customers
    WHERE customer_id IS NOT NULL
\"\""")

# Transform order items: Clean and calculate
con.execute("""\"
    CREATE TABLE silver_order_items AS
    SELECT
        order_id,
        product_id,
        seller_id,
        CAST(price AS DOUBLE) as price,
        CAST(freight_value AS DOUBLE) as freight,
        CAST(price AS DOUBLE) + CAST(freight_value AS DOUBLE) as total_value
    FROM bronze_order_items
    WHERE order_id IS NOT NULL
        AND product_id IS NOT NULL
\"\""")

print("Created silver tables with clean types")
print(f"  - silver_orders: {con.execute('SELECT COUNT(*) FROM silver_orders').fetchone()[0]} rows")
print(f"  - silver_customers: {con.execute('SELECT COUNT(*) FROM silver_customers').fetchone()[0]} rows")
print(f"  - silver_order_items: {con.execute('SELECT COUNT(*) FROM silver_order_items').fetchone()[0]} rows")""")

add_markdown_cell("""**What we did:**
- **Fixed types:** Timestamps parsed, numbers cast to numeric
- **Removed nulls:** Filtered out rows with missing critical IDs
- **Added calculated columns:** total_value = price + freight
- **Standardized names:** Dropped redundant prefixes

**Next:** Validate!

---

### Validation: Prove Data Quality""")

add_code_cell("""# VALIDATION: Assertions to check data quality
print("\\n=== VALIDATION: Checking Data Quality ===\\n")

# Validation 1: Primary key uniqueness
order_count = con.execute("SELECT COUNT(*) FROM silver_orders").fetchone()[0]
order_unique = con.execute("SELECT COUNT(DISTINCT order_id) FROM silver_orders").fetchone()[0]

print(f"‚úì Check 1: Order IDs unique?")
print(f"  Total rows: {order_count}")
print(f"  Unique IDs: {order_unique}")
assert order_count == order_unique, "Duplicate order IDs found!"
print("  ‚úÖ PASS: All order IDs are unique\\n")

# Validation 2: No null critical fields
null_order_ids = con.execute("SELECT COUNT(*) FROM silver_orders WHERE order_id IS NULL").fetchone()[0]
null_customer_ids = con.execute("SELECT COUNT(*) FROM silver_orders WHERE customer_id IS NULL").fetchone()[0]

print(f"‚úì Check 2: Required fields non-null?")
print(f"  NULL order_ids: {null_order_ids}")
print(f"  NULL customer_ids: {null_customer_ids}")
assert null_order_ids == 0, "NULL order IDs found!"
assert null_customer_ids == 0, "NULL customer IDs found!"
print("  ‚úÖ PASS: No nulls in required fields\\n")

# Validation 3: Foreign key integrity
order_ids_in_orders = con.execute("SELECT COUNT(DISTINCT order_id) FROM silver_orders").fetchone()[0]
order_ids_in_items = con.execute("SELECT COUNT(DISTINCT order_id) FROM silver_order_items").fetchone()[0]

# Check: All items belong to valid orders
orphaned_items = con.execute("""\"
    SELECT COUNT(*)
    FROM silver_order_items i
    LEFT JOIN silver_orders o ON i.order_id = o.order_id
    WHERE o.order_id IS NULL
\"\""").fetchone()[0]

print(f"‚úì Check 3: Foreign key integrity?")
print(f"  Orders with items: {order_ids_in_items}")
print(f"  Orphaned items (no matching order): {orphaned_items}")
assert orphaned_items == 0, f"Found {orphaned_items} items without matching orders!"
print("  ‚úÖ PASS: All items have valid orders\\n")

# Validation 4: Business rules
negative_prices = con.execute("SELECT COUNT(*) FROM silver_order_items WHERE price < 0").fetchone()[0]

print(f"‚úì Check 4: Business rules?")
print(f"  Negative prices: {negative_prices}")
assert negative_prices == 0, f"Found {negative_prices} negative prices!"
print("  ‚úÖ PASS: All prices are non-negative\\n")

print("="*60)
print("‚úÖ ALL VALIDATIONS PASSED - Silver layer is clean!")
print("="*60)""")

add_markdown_cell("""**What we validated:**
1. **Primary key uniqueness:** No duplicate order IDs
2. **Required fields:** No nulls in critical columns
3. **Foreign key integrity:** All order items link to valid orders
4. **Business rules:** Prices are non-negative

**If any assertion failed?**
- Pipeline stops immediately
- Error message tells you exactly what's wrong
- Fix issue in bronze or silver
- Re-run with confidence

**This is the magic:** Failures are loud and immediate, not silent and dangerous.

---

### Gold Layer: Business Metrics

**Goal:** Create aggregated tables ready for reporting.""")

add_code_cell("""# GOLD: Business metrics
print("=== GOLD LAYER: Business Metrics ===\\n")

# Gold table 1: Daily sales summary
con.execute("""\"
    CREATE TABLE gold_daily_sales AS
    SELECT
        CAST(o.order_date AS DATE) as date,
        COUNT(DISTINCT o.order_id) as num_orders,
        COUNT(DISTINCT o.customer_id) as num_customers,
        SUM(i.total_value) as total_revenue,
        AVG(i.total_value) as avg_order_value
    FROM silver_orders o
    INNER JOIN silver_order_items i ON o.order_id = i.order_id
    WHERE o.order_date IS NOT NULL
    GROUP BY CAST(o.order_date AS DATE)
    ORDER BY date
\"\""")

print("Created gold_daily_sales table")
print("\\nSample data:")
result = con.execute("SELECT * FROM gold_daily_sales LIMIT 5").df()
display(result)

# Gold table 2: Customer summary
con.execute("""\"
    CREATE TABLE gold_customer_summary AS
    SELECT
        c.customer_id,
        c.state,
        COUNT(DISTINCT o.order_id) as num_orders,
        SUM(i.total_value) as lifetime_value,
        MIN(o.order_date) as first_order_date,
        MAX(o.order_date) as last_order_date
    FROM silver_customers c
    INNER JOIN silver_orders o ON c.customer_id = o.customer_id
    INNER JOIN silver_order_items i ON o.order_id = i.order_id
    GROUP BY c.customer_id, c.state
\"\""")

print("\\nCreated gold_customer_summary table")
print("Sample data:")
result2 = con.execute("SELECT * FROM gold_customer_summary LIMIT 5").df()
display(result2)

print("\\n‚úÖ Gold layer complete: Metrics ready for dashboards!")""")

add_markdown_cell("""**What we created:**
- **Daily sales:** Aggregated by date for time series
- **Customer summary:** Lifetime value per customer

**Why gold layer?**
- Pre-computed aggregations are fast
- Stakeholders query gold, not raw data
- Can create multiple gold tables from same silver
- Denormalized for dashboard performance

---

### The Complete Pipeline""")

add_code_cell("""# Summary: Show the full pipeline
print("=== PIPELINE SUMMARY ===\\n")

print("BRONZE ‚Üí SILVER ‚Üí GOLD")
print()
print("Bronze (Raw):")
print("  ‚Ä¢ olist_orders_subset.csv ‚Üí bronze_orders")
print("  ‚Ä¢ olist_customers_subset.csv ‚Üí bronze_customers")
print("  ‚Ä¢ olist_order_items_subset.csv ‚Üí bronze_order_items")
print()
print("Silver (Clean & Validated):")
print("  ‚Ä¢ bronze_orders ‚Üí silver_orders (typed, validated)")
print("  ‚Ä¢ bronze_customers ‚Üí silver_customers (clean)")
print("  ‚Ä¢ bronze_order_items ‚Üí silver_order_items (calculated)")
print("  ‚Ä¢ ‚úÖ 4 validations passed")
print()
print("Gold (Business Metrics):")
print("  ‚Ä¢ silver_* ‚Üí gold_daily_sales (aggregated by date)")
print("  ‚Ä¢ silver_* ‚Üí gold_customer_summary (per customer)")
print()
print("Tomorrow: Re-run with new data, validations catch problems!")""")

add_markdown_cell("""---

## 4. Key Principles

### 1. Idempotency

> **"Running the pipeline twice gives the same result."**

**Bad (not idempotent):**
```python
# Appends every time you run it
gold = pd.concat([gold, new_data])
```

If you run this twice, you get duplicates!

**Good (idempotent):**
```python
# Recreates from scratch
con.execute("DROP TABLE IF EXISTS gold_daily_sales")
con.execute("CREATE TABLE gold_daily_sales AS SELECT ...")
```

Running twice gives the same result.

**Why it matters:**
- Can re-run safely after failures
- Can refresh data without cleanup
- Deterministic results

---

### 2. Fail Fast

> **"If data is bad, stop immediately with a clear error."**

**Bad:**
```python
# Silently continues with bad data
if df['price'].min() < 0:
    print("Warning: negative prices")
# ... continues anyway
```

Analyst sees warning (maybe), results are wrong.

**Good:**
```python
# Stops with clear error
assert df['price'].min() >= 0, "Negative prices found - fix source data!"
```

Pipeline stops, error is loud, issue must be fixed.

---

### 3. Document Assumptions

Every validation is documentation:

```python
# This assertion says: "We assume order IDs are unique"
assert df['order_id'].is_unique, "Duplicate order IDs"

# This says: "We assume prices are USD and non-negative"
assert (df['price'] >= 0).all(), "Negative prices"

# This says: "We assume all items belong to valid orders"
assert orphan_count == 0, f"{orphan_count} orphaned items"
```

Six months later, someone can read your code and understand your assumptions.

---""")

# ============================================================================
# SECTION 4: REAL-WORLD DATA ISSUES
# ============================================================================

add_markdown_cell("""## 5. Real-World Data Survival Tips

### Date Handling

Dates are deceptively hard!""")

add_code_cell("""# Date horror stories
print("=== DATE HANDLING ===\\n")

# Example: Mixed date formats
mixed_dates = pd.DataFrame({
    'order_id': ['A', 'B', 'C'],
    'date_string': ['2024-01-15', '01/16/2024', '2024.01.17']
})

print("Mixed date formats:")
print(mixed_dates)

# Wrong: Try to parse all at once
try:
    mixed_dates['date'] = pd.to_datetime(mixed_dates['date_string'])
    print("\\nParsed successfully (lucky!)")
    print(mixed_dates)
except Exception as e:
    print(f"\\n‚ùå Error: {e}")

# Better: Standardize first, then parse
print("\\nBetter approach: Check what format you have")
print("For Olist data, we have ISO 8601 (YYYY-MM-DD HH:MM:SS)")
print("DuckDB's TRY_CAST handles this gracefully:")

result = con.execute("""\"
    SELECT
        order_purchase_timestamp as original,
        TRY_CAST(order_purchase_timestamp AS TIMESTAMP) as parsed,
        CASE
            WHEN TRY_CAST(order_purchase_timestamp AS TIMESTAMP) IS NULL
            THEN 'FAILED'
            ELSE 'OK'
        END as parse_status
    FROM bronze_orders
    LIMIT 5
\"\""").df()
display(result)

print("\\nüí° Tips for dates:")
print("  ‚Ä¢ Always check format before parsing")
print("  ‚Ä¢ Use TRY_CAST or pd.to_datetime(errors='coerce')")
print("  ‚Ä¢ Validate date ranges (no dates in future, no dates in 1900)")
print("  ‚Ä¢ Watch for timezone issues")""")

add_markdown_cell("""---

### Type Checking""")

add_code_cell("""# Type checking example
print("=== TYPE CHECKING ===\\n")

# Check types in silver layer
print("Silver layer column types:")
result = con.execute("""\"
    SELECT
        column_name,
        data_type
    FROM information_schema.columns
    WHERE table_name = 'silver_order_items'
\"\""").df()
display(result)

print("\\nValidate types match expectations:")
expected_types = {
    'order_id': 'VARCHAR',
    'product_id': 'VARCHAR',
    'price': 'DOUBLE',
    'freight': 'DOUBLE'
}

actual_types = dict(zip(result['column_name'], result['data_type']))

for col, expected in expected_types.items():
    actual = actual_types.get(col, 'MISSING')
    status = '‚úÖ' if actual == expected else '‚ùå'
    print(f"  {status} {col}: expected {expected}, got {actual}")

print("\\nüí° Tips for types:")
print("  ‚Ä¢ Always check df.dtypes after loading")
print("  ‚Ä¢ Numbers stored as strings? Clean and convert")
print("  ‚Ä¢ Watch for mixed types in columns")
print("  ‚Ä¢ Use TRY_CAST to handle errors gracefully")""")

add_markdown_cell("""---

### NULL Handling Strategy""")

add_code_cell("""# NULL handling
print("=== NULL HANDLING ===\\n")

# Check NULL counts in silver
result = con.execute("""\"
    SELECT
        COUNT(*) as total_rows,
        COUNT(order_id) as non_null_order_id,
        COUNT(customer_id) as non_null_customer_id,
        COUNT(order_date) as non_null_order_date,
        COUNT(delivery_date) as non_null_delivery_date
    FROM silver_orders
\"\""").df()

print("NULL counts in silver_orders:")
display(result)

null_delivery = result['total_rows'].values[0] - result['non_null_delivery_date'].values[0]
print(f"\\n{null_delivery} orders have NULL delivery_date")
print("\\nInterpretation:")
print("  ‚Ä¢ These orders haven't been delivered yet")
print("  ‚Ä¢ NULL here means 'not yet', not 'missing data'")
print("  ‚Ä¢ Strategy: Keep as NULL, exclude from delivery time analysis")

print("\\nüí° Tips for NULLs:")
print("  ‚Ä¢ Understand what NULL means (not applicable? unknown? not yet?)")
print("  ‚Ä¢ Document your handling strategy")
print("  ‚Ä¢ Use COALESCE in SQL or fillna() in pandas")
print("  ‚Ä¢ Remember: COUNT() excludes NULLs, SUM() excludes NULLs")""")

add_markdown_cell("""---

### When to Use SQL vs Python""")

add_code_cell("""# When to use SQL vs Python
print("=== SQL vs PYTHON ===\\n")

print("Use SQL when:")
print("  ‚úÖ Filtering rows (WHERE)")
print("  ‚úÖ Joining tables (JOIN)")
print("  ‚úÖ Grouping and aggregating (GROUP BY)")
print("  ‚úÖ Sorting (ORDER BY)")
print("  ‚úÖ Set operations (UNION, INTERSECT)")
print()
print("Why? Databases optimize these operations, vectorized, fast")

print("\\nUse Python when:")
print("  ‚úÖ Complex string manipulation (regex, parsing)")
print("  ‚úÖ API calls / web scraping")
print("  ‚úÖ Machine learning")
print("  ‚úÖ Custom business logic (complicated IF-THEN rules)")
print("  ‚úÖ Visualization")
print()
print("Why? More flexible, richer libraries")

print("\\nüí° Pro tip:")
print("  Start with SQL (filter, aggregate)")
print("  Then Python (complex logic, ML, viz)")
print("  Example: Extract customers with SQL ‚Üí Python ML model ‚Üí SQL to store scores")""")

add_markdown_cell("""---

## 6. Work Habits

### 1. Restart & Run All

**Before committing any notebook:**
```
Kernel ‚Üí Restart & Run All Cells
```

Ensures reproducibility. Your "it works on my machine" doesn't count!

---

### 2. Small, Frequent Commits

**Good:**
```
git commit -m "Add bronze layer ingestion"
git commit -m "Add silver layer with validation"
git commit -m "Add gold daily sales table"
```

**Bad:**
```
git commit -m "Everything done"  # 500 lines changed
```

Small commits make debugging easier.

---

### 3. Read the Docs

Don't guess! Official documentation is authoritative:
- **DuckDB:** https://duckdb.org/docs/
- **Pandas:** https://pandas.pydata.org/docs/
- **Python:** https://docs.python.org/3/

Pro tip: Search "duckdb try_cast" instead of guessing syntax.

---

### 4. Rubber Duck Debugging

Stuck? Explain your problem out loud (to a rubber duck, or colleague).

Often, articulating the problem reveals the solution.

**Example:**
- "My validation fails... I'm checking if order_id is unique... wait, am I checking the right table? OH! I'm checking bronze instead of silver!"

---

## Summary

### Key Takeaways

1. **Pipeline pattern:** Bronze (raw) ‚Üí Silver (clean) ‚Üí Gold (metrics)
2. **Validations:** Assertions catch problems early and loudly
3. **Idempotency:** Re-running gives same result
4. **Dates:** Always parse explicitly and validate
5. **Types:** Check and convert early
6. **NULLs:** Understand meaning, document strategy
7. **SQL vs Python:** Use each for its strengths
8. **Work habits:** Restart & Run All, small commits, read docs

### Why This Matters

**Without pipelines:**
- Manual work every time
- Inconsistent results
- Errors discovered late
- Hard to debug
- Can't reproduce

**With pipelines:**
- Automated, repeatable
- Validated at every step
- Errors caught immediately
- Easy to debug (check each layer)
- Fully reproducible

**You're not just analyzing data‚Äîyou're building infrastructure.**

---

## Next: In-Class Exercise

**Your turn!**

Build a mini-pipeline with the smaller Olist subset:
1. Bronze: Load raw data
2. Silver: Clean, validate (write 2 assertions)
3. Gold: Create 2-3 summary metrics
4. Document: Write risk note (assumptions & limitations)

**Time:** 15 minutes
**Notebook:** `day3_exercise_mini_pipeline.ipynb`

**Let's build!** üöÄ""")

# ============================================================================
# CREATE NOTEBOOK
# ============================================================================

notebook = {
    "cells": cells,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

# Save notebook
output_path = Path("notebooks/day3/day3_block_a_pipelines_and_validations.ipynb")
output_path.parent.mkdir(parents=True, exist_ok=True)

with open(output_path, 'w') as f:
    json.dump(notebook, f, indent=2)

print(f"‚úÖ Created: {output_path}")
print(f"   Total cells: {len(cells)}")
print(f"   Ready for teaching!")
