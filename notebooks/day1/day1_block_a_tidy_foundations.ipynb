{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1, Block A: Data Thinking & Tidy Foundations\n",
    "\n",
    "**Duration:** 90 minutes (13:30â€“15:10)  \n",
    "**Course:** ECBS5294 - Introduction to Data Science: Working with Data  \n",
    "**Instructor:** Eduardo AriÃ±o de la Rubia\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. Articulate the **strategic value** of data structure\n",
    "2. State the **three rules** of tidy data and recognize violations\n",
    "3. Identify the **five common messiness patterns** in real datasets\n",
    "4. Designate and validate a **primary key (UID)** for a dataset\n",
    "5. Recognize common **type pitfalls** (dates, floats, strings-as-numbers)\n",
    "6. Handle **missing values** appropriately\n",
    "7. Transform a messy dataset into a tidy format\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Power of Data Structure\n",
    "\n",
    "### Why Does Data Structure Matter?\n",
    "\n",
    "> **\"When you define how data is structured, you're not just organizing informationâ€”you're defining the language your entire organization will use to make decisions.\"**\n",
    "\n",
    "Think about it:\n",
    "\n",
    "- Every **\"customer,\" \"order,\" \"product\"** in your database becomes a **noun** the business rallies around\n",
    "- Every **\"purchase,\" \"return,\" \"review\"** becomes a **verb** that drives metrics\n",
    "- You're laying the foundation for how people **think, talk, and measure success**\n",
    "\n",
    "### Example: What Is a Customer?\n",
    "\n",
    "This isn't just a technical question:\n",
    "- One per **email address**?\n",
    "- One per **household**?\n",
    "- One per **device**?\n",
    "\n",
    "**Each choice has business implications:**\n",
    "- Affects customer count metrics\n",
    "- Changes how you measure retention\n",
    "- Impacts marketing campaigns\n",
    "- Determines privacy/GDPR compliance\n",
    "\n",
    "**You're not just structuring dataâ€”you're making strategic business decisions.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief History: Why Do Databases Exist?\n",
    "\n",
    "####  Before Computers (Pre-1960s)\n",
    "- Data stored in **filing cabinets**\n",
    "- Physical cards, journals, ledgers\n",
    "- Problems: Space, hard to search, hard to back up\n",
    "\n",
    "#### Early Databases (1960s)\n",
    "- First computerized databases emerged\n",
    "- **Navigational databases**: Hierarchical and network models\n",
    "- Problem: Had to know the question in advance to design the structure\n",
    "\n",
    "#### The Revolution: E.F. Codd's Relational Model (1970)\n",
    "- IBM computer scientist Edgar F. Codd published a groundbreaking paper\n",
    "- **Key Innovation:** Build cross-linked tables where you **store each fact once**\n",
    "\n",
    "**The Problem Codd Solved:**\n",
    "1. **Disk space was expensive** â†’ Eliminate redundancy\n",
    "2. **Flexibility was limited** â†’ Answer **any question** if the data exists\n",
    "3. **Schema independence** â†’ Logical structure separate from physical storage\n",
    "\n",
    "> **\"Store each fact once, answer any question.\"**\n",
    "\n",
    "#### Modern Era (1970s-Today)\n",
    "- **1979:** Oracle - first commercial relational database\n",
    "- **1980s-90s:** Relational databases became dominant\n",
    "- **SQL** (Structured Query Language) became the standard\n",
    "\n",
    "**Why This Matters:**\n",
    "When you learn SQL and work with databases, you're using technology that solved one of computing's fundamental problems. Understanding **tables, keys, and relationships** is the foundation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Three Rules of Tidy Data\n",
    "\n",
    "### Who is Hadley Wickham?\n",
    "\n",
    "- Statistician at Posit (formerly RStudio)\n",
    "- Created the \"tidyverse\" - influential R packages\n",
    "- Published influential paper: **\"Tidy Data\"** (2014)\n",
    "- These principles work across **all tools** (Python, R, SQL, even Excel!)\n",
    "\n",
    "### The Three Rules\n",
    "\n",
    "A dataset is **tidy** if:\n",
    "\n",
    "1. **Each variable is a column**\n",
    "2. **Each observation is a row**\n",
    "3. **Each value is a cell**\n",
    "\n",
    "That's it! Simple to state, but powerful in practice.\n",
    "\n",
    "### Why These Rules Matter\n",
    "\n",
    "- **Consistency** makes tools easier to learn and use\n",
    "- **Vectorized operations** work naturally (add up a column)\n",
    "- **Analysis becomes intuitive** (filter rows, group columns)\n",
    "- **Joining data becomes possible** (need consistent row-level observations)\n",
    "\n",
    "### Example: Messy vs. Tidy\n",
    "\n",
    "Let's look at some sales data to see what \"tidy\" really means.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.224382Z",
     "iopub.status.busy": "2025-10-13T14:25:24.224321Z",
     "iopub.status.idle": "2025-10-13T14:25:24.426139Z",
     "shell.execute_reply": "2025-10-13T14:25:24.424998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarnings for cleaner output in teaching environment\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Messy Example: Years as column names\n",
    "messy_sales = pd.DataFrame({\n",
    "    'product': ['Widget', 'Gadget', 'Doohickey'],\n",
    "    '2021': [100, 150, 200],\n",
    "    '2022': [120, 160, 190],\n",
    "    '2023': [140, 180, 210]\n",
    "})\n",
    "\n",
    "print(\"MESSY: Years as column names\")\n",
    "print(messy_sales)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Tidy Example: Year as a variable\n",
    "tidy_sales = pd.DataFrame({\n",
    "    'product': ['Widget', 'Widget', 'Widget', 'Gadget', 'Gadget', 'Gadget', 'Doohickey', 'Doohickey', 'Doohickey'],\n",
    "    'year': [2021, 2022, 2023, 2021, 2022, 2023, 2021, 2022, 2023],\n",
    "    'sales': [100, 120, 140, 150, 160, 180, 200, 190, 210]\n",
    "})\n",
    "\n",
    "print(\"TIDY: Year as a variable (column)\")\n",
    "print(tidy_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Check the Rules\n",
    "\n",
    "**Messy version:**\n",
    "- âŒ **Rule 1 violated**: Column headers (2021, 2022, 2023) are **values**, not **variable names**\n",
    "- âŒ **Analysis problem**: How do you add 2024 data? How do you filter by year? How do you plot trends?\n",
    "\n",
    "**Tidy version:**\n",
    "- âœ… **Rule 1**: Each variable (product, year, sales) is a column\n",
    "- âœ… **Rule 2**: Each observation (product-year combination) is a row\n",
    "- âœ… **Rule 3**: Each value is in its own cell\n",
    "- âœ… **Analysis is easy**: Filter by year, group by product, plot trends, add new years\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.446640Z",
     "iopub.status.busy": "2025-10-13T14:25:24.446315Z",
     "iopub.status.idle": "2025-10-13T14:25:24.452948Z",
     "shell.execute_reply": "2025-10-13T14:25:24.452212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Why tidy is better for analysis\n",
    "print(\"Calculate average sales per year (try this with messy version!):\")\n",
    "print(tidy_sales.groupby('year')['sales'].mean())\n",
    "\n",
    "print(\"\\nFilter to just 2023:\")\n",
    "print(tidy_sales[tidy_sales['year'] == 2023])\n",
    "\n",
    "print(\"\\nAdd 2024 data: Just add new rows!\")\n",
    "# new_data = pd.DataFrame({'product': ['Widget'], 'year': [2024], 'sales': [150]})\n",
    "# tidy_sales = pd.concat([tidy_sales, new_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Check: Is This Tidy?\n",
    "\n",
    "Look at the following examples and ask yourself:\n",
    "1. Is each variable a column?\n",
    "2. Is each observation a row?\n",
    "3. Is each value in its own cell?\n",
    "\n",
    "We'll discuss these together!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.455644Z",
     "iopub.status.busy": "2025-10-13T14:25:24.454931Z",
     "iopub.status.idle": "2025-10-13T14:25:24.460815Z",
     "shell.execute_reply": "2025-10-13T14:25:24.460284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Is this tidy?\n",
    "example2 = pd.DataFrame({\n",
    "    'student_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'math_score': [85, 92, 78],\n",
    "    'english_score': [90, 88, 95]\n",
    "})\n",
    "\n",
    "print(\"Example 2:\")\n",
    "print(example2)\n",
    "print(\"\\nIs this tidy? Think about it...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It depends on your analysis!\n",
    "\n",
    "- If each student is an observation â†’ **Tidy** âœ…\n",
    "- If each test score is an observation â†’ **Not tidy** âŒ (subjects are column names, not a variable)\n",
    "\n",
    "**Tidy version for score-level analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.463163Z",
     "iopub.status.busy": "2025-10-13T14:25:24.462695Z",
     "iopub.status.idle": "2025-10-13T14:25:24.467590Z",
     "shell.execute_reply": "2025-10-13T14:25:24.467137Z"
    }
   },
   "outputs": [],
   "source": [
    "example2_tidy = pd.DataFrame({\n",
    "    'student_id': [1, 1, 2, 2, 3, 3],\n",
    "    'name': ['Alice', 'Alice', 'Bob', 'Bob', 'Charlie', 'Charlie'],\n",
    "    'subject': ['math', 'english', 'math', 'english', 'math', 'english'],\n",
    "    'score': [85, 90, 92, 88, 78, 95]\n",
    "})\n",
    "\n",
    "print(\"Tidy version (if each score is an observation):\")\n",
    "print(example2_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "> **\"Tidy datasets are all alike, but every messy dataset is messy in its own way.\"** â€“ Hadley Wickham\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Five Common Problems\n",
    "\n",
    "Hadley Wickham identified five common ways datasets become messy:\n",
    "\n",
    "### Problem 1: Column Headers are Values, Not Variable Names\n",
    "\n",
    "**Example:** Years, categories, or measurements as column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.469814Z",
     "iopub.status.busy": "2025-10-13T14:25:24.469504Z",
     "iopub.status.idle": "2025-10-13T14:25:24.473768Z",
     "shell.execute_reply": "2025-10-13T14:25:24.472949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Problem 1 Example\n",
    "problem1 = pd.DataFrame({\n",
    "    'religion': ['Buddhist', 'Catholic', 'Protestant'],\n",
    "    '<$10k': [27, 418, 732],\n",
    "    '$10-20k': [21, 617, 670],\n",
    "    '$20-30k': [30, 732, 638]\n",
    "})\n",
    "\n",
    "print(\"Problem 1: Income brackets as column names\")\n",
    "print(problem1)\n",
    "print(\"\\nIssue: Can't easily filter by income bracket, plot income distribution, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Make income bracket a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.476255Z",
     "iopub.status.busy": "2025-10-13T14:25:24.476103Z",
     "iopub.status.idle": "2025-10-13T14:25:24.481938Z",
     "shell.execute_reply": "2025-10-13T14:25:24.480673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution using pd.melt()\n",
    "problem1_tidy = problem1.melt(\n",
    "    id_vars=['religion'],\n",
    "    var_name='income_bracket',\n",
    "    value_name='count'\n",
    ")\n",
    "\n",
    "print(\"Solution: Income bracket as a variable\")\n",
    "print(problem1_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Multiple Variables Stored in One Column\n",
    "\n",
    "**Example:** Combining attributes into single column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.484248Z",
     "iopub.status.busy": "2025-10-13T14:25:24.483901Z",
     "iopub.status.idle": "2025-10-13T14:25:24.488348Z",
     "shell.execute_reply": "2025-10-13T14:25:24.487659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Problem 2 Example\n",
    "problem2 = pd.DataFrame({\n",
    "    'country': ['USA', 'Canada', 'Mexico'],\n",
    "    'male_under_18': [1000, 800, 600],\n",
    "    'male_over_18': [2000, 1500, 1200],\n",
    "    'female_under_18': [950, 780, 620],\n",
    "    'female_over_18': [2100, 1550, 1180]\n",
    "})\n",
    "\n",
    "print(\"Problem 2: Gender and age combined in column names\")\n",
    "print(problem2)\n",
    "print(\"\\nIssue: Can't analyze by gender alone or age alone easily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Separate gender and age into distinct variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.490304Z",
     "iopub.status.busy": "2025-10-13T14:25:24.490158Z",
     "iopub.status.idle": "2025-10-13T14:25:24.497533Z",
     "shell.execute_reply": "2025-10-13T14:25:24.496683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Melt then split\n",
    "problem2_melted = problem2.melt(id_vars=['country'], var_name='demographic', value_name='count')\n",
    "problem2_melted[['gender', 'age_group']] = problem2_melted['demographic'].str.split('_', n=1, expand=True)\n",
    "problem2_tidy = problem2_melted[['country', 'gender', 'age_group', 'count']]\n",
    "\n",
    "print(\"Solution: Gender and age as separate variables\")\n",
    "print(problem2_tidy.head(8))\n",
    "print(\"\\nNow we can analyze by gender, age, or both!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Variables Stored in Both Rows AND Columns\n",
    "\n",
    "**Example:** Matrix-style data where both axes represent variables\n",
    "\n",
    "This is less common and more complex. Key idea: If you have a \"pivot table\" style layout, you likely need to unpivot it.\n",
    "\n",
    "**We'll skip detailed examples** for now (confusing for day 1). Just know: If your data looks like a matrix or spreadsheet pivot table, it probably needs tidying.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 4: Multiple Types of Observational Units in Same Table\n",
    "\n",
    "**Example:** Mixing customer info with transaction info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.499630Z",
     "iopub.status.busy": "2025-10-13T14:25:24.499547Z",
     "iopub.status.idle": "2025-10-13T14:25:24.502099Z",
     "shell.execute_reply": "2025-10-13T14:25:24.501896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Problem 4 Example\n",
    "problem4 = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN003'],\n",
    "    'customer_name': ['Alice', 'Bob', 'Alice'],\n",
    "    'customer_email': ['alice@example.com', 'bob@example.com', 'alice@example.com'],\n",
    "    'product': ['Widget', 'Gadget', 'Doohickey'],\n",
    "    'price': [10.00, 15.00, 20.00]\n",
    "})\n",
    "\n",
    "print(\"Problem 4: Customer info repeated on every transaction\")\n",
    "print(problem4)\n",
    "print(\"\\nIssues:\")\n",
    "print(\"- Alice's email is stored twice (redundancy)\")\n",
    "print(\"- If Alice changes email, must update multiple rows\")\n",
    "print(\"- Wasted storage space\")\n",
    "print(\"- Risk of inconsistency (what if one row has old email?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Separate into two tables linked by customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.503174Z",
     "iopub.status.busy": "2025-10-13T14:25:24.503108Z",
     "iopub.status.idle": "2025-10-13T14:25:24.505601Z",
     "shell.execute_reply": "2025-10-13T14:25:24.505418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Two tables\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2],\n",
    "    'customer_name': ['Alice', 'Bob'],\n",
    "    'customer_email': ['alice@example.com', 'bob@example.com']\n",
    "})\n",
    "\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN003'],\n",
    "    'customer_id': [1, 2, 1],\n",
    "    'product': ['Widget', 'Gadget', 'Doohickey'],\n",
    "    'price': [10.00, 15.00, 20.00]\n",
    "})\n",
    "\n",
    "print(\"Solution: Separate tables\")\n",
    "print(\"\\nCustomers table:\")\n",
    "print(customers)\n",
    "print(\"\\nTransactions table:\")\n",
    "print(transactions)\n",
    "print(\"\\nAlice's email stored ONCE. Link via customer_id. This is 'normalization'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Single Observational Unit Spread Across Multiple Tables\n",
    "\n",
    "**Example:** Data split by time period or category into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.506505Z",
     "iopub.status.busy": "2025-10-13T14:25:24.506435Z",
     "iopub.status.idle": "2025-10-13T14:25:24.508789Z",
     "shell.execute_reply": "2025-10-13T14:25:24.508578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Problem 5 Example\n",
    "sales_jan = pd.DataFrame({\n",
    "    'product': ['Widget', 'Gadget'],\n",
    "    'sales': [100, 150]\n",
    "})\n",
    "\n",
    "sales_feb = pd.DataFrame({\n",
    "    'product': ['Widget', 'Gadget'],\n",
    "    'sales': [120, 160]\n",
    "})\n",
    "\n",
    "print(\"Problem 5: Data split across files\")\n",
    "print(\"\\nsales_jan.csv:\")\n",
    "print(sales_jan)\n",
    "print(\"\\nsales_feb.csv:\")\n",
    "print(sales_feb)\n",
    "print(\"\\nIssue: Can't easily query across months, calculate totals, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Combine into one table with a month variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.509727Z",
     "iopub.status.busy": "2025-10-13T14:25:24.509670Z",
     "iopub.status.idle": "2025-10-13T14:25:24.511909Z",
     "shell.execute_reply": "2025-10-13T14:25:24.511703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Combine with month column\n",
    "sales_jan['month'] = 'January'\n",
    "sales_feb['month'] = 'February'\n",
    "sales_all = pd.concat([sales_jan, sales_feb], ignore_index=True)\n",
    "\n",
    "print(\"Solution: One table with month variable\")\n",
    "print(sales_all)\n",
    "print(\"\\nNow we can easily:\")\n",
    "print(\"- Query any month: sales_all[sales_all['month'] == 'January']\")\n",
    "print(\"- Calculate totals: sales_all.groupby('product')['sales'].sum()\")\n",
    "print(\"- Add March data: Just append new rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Five Problems\n",
    "\n",
    "1. **Column headers are values** â†’ Make them a variable\n",
    "2. **Multiple variables in one column** â†’ Split into separate variables\n",
    "3. **Variables in rows AND columns** â†’ Reshape (complex, skip for now)\n",
    "4. **Multiple observational units in one table** â†’ Split into separate tables\n",
    "5. **Single observational unit across multiple tables** â†’ Combine into one table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Primary Keys & Identity\n",
    "\n",
    "### What is a Primary Key?\n",
    "\n",
    "> **\"Every entity needs a unique identifierâ€”a primary key (UID). This is the backbone of data integrity.\"**\n",
    "\n",
    "A **primary key** uniquely identifies each row in your dataset.\n",
    "\n",
    "### What Makes a Good Primary Key?\n",
    "\n",
    "- **Unique**: No duplicates\n",
    "- **Non-null**: Every row must have one\n",
    "- **Stable**: Doesn't change over time\n",
    "- **Single-purpose**: Exists to identify, not to describe\n",
    "\n",
    "### Types of Primary Keys\n",
    "\n",
    "**Natural Key:** Something inherent to the entity\n",
    "- Email address (for users)\n",
    "- ISBN (for books)\n",
    "- SSN (for peopleâ€”privacy concerns!)\n",
    "- License plate (for vehicles)\n",
    "\n",
    "**Surrogate Key:** Made up for database purposes\n",
    "- customer_id: 1, 2, 3, ...\n",
    "- order_id: ORD001, ORD002, ...\n",
    "- Auto-incrementing integers\n",
    "\n",
    "**Composite Key:** Multiple columns together\n",
    "- (store_id, date) for daily store sales\n",
    "- (student_id, course_id) for enrollments\n",
    "\n",
    "### Why Primary Keys Matter\n",
    "\n",
    "**Business Impact:**\n",
    "- Foundation for joining tables (which customer made which purchase?)\n",
    "- Ensures you can uniquely identify each observation\n",
    "- Prevents duplicate records (customer entered twice?)\n",
    "- Essential for updates (which record do I change?)\n",
    "- Enables tracking over time (same customer, different transactions)\n",
    "\n",
    "**Red Flags:**\n",
    "- No obvious UID â†’ Need to create one or investigate grain of data\n",
    "- Multiple rows with same UID â†’ Data quality problem, investigate!\n",
    "- UID with NULLs â†’ Missing identifiers, data incomplete\n",
    "\n",
    "### Example: Validating a Primary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.513009Z",
     "iopub.status.busy": "2025-10-13T14:25:24.512950Z",
     "iopub.status.idle": "2025-10-13T14:25:24.515168Z",
     "shell.execute_reply": "2025-10-13T14:25:24.514983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example transactions with good primary key\n",
    "good_transactions = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN003'],\n",
    "    'customer_id': [101, 102, 101],\n",
    "    'amount': [50.00, 75.00, 30.00]\n",
    "})\n",
    "\n",
    "print(\"Good transactions table:\")\n",
    "print(good_transactions)\n",
    "\n",
    "# Validate primary key\n",
    "print(\"\\n=== Primary Key Validation ===\")\n",
    "print(f\"Is transaction_id unique? {good_transactions['transaction_id'].is_unique}\")\n",
    "print(f\"Any NULLs in transaction_id? {good_transactions['transaction_id'].isna().any()}\")\n",
    "print(\"âœ… transaction_id is a valid primary key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.515989Z",
     "iopub.status.busy": "2025-10-13T14:25:24.515935Z",
     "iopub.status.idle": "2025-10-13T14:25:24.518384Z",
     "shell.execute_reply": "2025-10-13T14:25:24.518204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example with BAD primary key (duplicates)\n",
    "bad_transactions = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN001'],  # Duplicate!\n",
    "    'customer_id': [101, 102, 103],\n",
    "    'amount': [50.00, 75.00, 30.00]\n",
    "})\n",
    "\n",
    "print(\"\\nBad transactions table (has duplicate IDs):\")\n",
    "print(bad_transactions)\n",
    "\n",
    "# Validate\n",
    "print(\"\\n=== Primary Key Validation ===\")\n",
    "print(f\"Is transaction_id unique? {bad_transactions['transaction_id'].is_unique}\")\n",
    "print(\"âŒ PROBLEM: Duplicate transaction IDs found!\")\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = bad_transactions[bad_transactions.duplicated(subset=['transaction_id'], keep=False)]\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always Validate With Assertions\n",
    "\n",
    "**Best practice:** Add assertions to your code to prove data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.519340Z",
     "iopub.status.busy": "2025-10-13T14:25:24.519288Z",
     "iopub.status.idle": "2025-10-13T14:25:24.521255Z",
     "shell.execute_reply": "2025-10-13T14:25:24.521039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Assertions for primary key validation\n",
    "def validate_primary_key(df, key_column):\n",
    "    \"\"\"\n",
    "    Validate that a column is a proper primary key.\n",
    "    Raises AssertionError if validation fails.\n",
    "    \"\"\"\n",
    "    # Check for uniqueness\n",
    "    assert df[key_column].is_unique, f\"Duplicate values found in {key_column}\"\n",
    "    \n",
    "    # Check for NULLs\n",
    "    assert df[key_column].notna().all(), f\"NULL values found in {key_column}\"\n",
    "    \n",
    "    print(f\"âœ… {key_column} is a valid primary key\")\n",
    "    print(f\"   - {len(df)} unique values\")\n",
    "    print(f\"   - No NULLs\")\n",
    "\n",
    "# Test it\n",
    "try:\n",
    "    validate_primary_key(good_transactions, 'transaction_id')\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ Validation failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    validate_primary_key(bad_transactions, 'transaction_id')\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "**Always designate and validate your primary key!**\n",
    "\n",
    "- Use assertions to prove uniqueness and non-null\n",
    "- If you don't have a natural key, create a surrogate key\n",
    "- Document what the key represents\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Types & Common Pitfalls\n",
    "\n",
    "> **\"Computers don't understand contextâ€”you must tell them what each column means. The wrong type silently breaks calculations.\"**\n",
    "\n",
    "### Why Types Matter\n",
    "\n",
    "- Enable correct operations (math on numbers, not strings)\n",
    "- Enable validation (dates must be valid dates)\n",
    "- Enable efficiency (numbers stored efficiently, not as text)\n",
    "- Enable analysis (group by categories, aggregate numbers)\n",
    "\n",
    "### Common Type Pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 1: Dates\n",
    "\n",
    "Dates are surprisingly hard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.522161Z",
     "iopub.status.busy": "2025-10-13T14:25:24.522105Z",
     "iopub.status.idle": "2025-10-13T14:25:24.523928Z",
     "shell.execute_reply": "2025-10-13T14:25:24.523744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pitfall 1: Mixed date formats\n",
    "messy_dates = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN003'],\n",
    "    'date': ['2024-01-15', '01/16/2024', 'January 17, 2024']\n",
    "})\n",
    "\n",
    "print(\"Messy dates (multiple formats):\")\n",
    "print(messy_dates)\n",
    "print(f\"\\nDate column type: {messy_dates['date'].dtype}\")\n",
    "print(\"Problem: These are strings, not dates! Can't do date math or filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.524722Z",
     "iopub.status.busy": "2025-10-13T14:25:24.524664Z",
     "iopub.status.idle": "2025-10-13T14:25:24.527530Z",
     "shell.execute_reply": "2025-10-13T14:25:24.527344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Parse dates explicitly\n",
    "# Note: This may fail if formats are truly mixed in one column\n",
    "# In real life, you'd need to handle each format\n",
    "\n",
    "clean_dates = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN003'],\n",
    "    'date': ['2024-01-15', '2024-01-16', '2024-01-17']\n",
    "})\n",
    "\n",
    "clean_dates['date'] = pd.to_datetime(clean_dates['date'])\n",
    "\n",
    "print(\"Clean dates:\")\n",
    "print(clean_dates)\n",
    "print(f\"\\nDate column type: {clean_dates['date'].dtype}\")\n",
    "print(\"âœ… Now we can do date math!\")\n",
    "\n",
    "# Examples of what we can do now\n",
    "print(f\"\\nLatest date: {clean_dates['date'].max()}\")\n",
    "print(f\"Days between first and last: {(clean_dates['date'].max() - clean_dates['date'].min()).days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Lessons for Dates:**\n",
    "- Always parse explicitly with `pd.to_datetime()`\n",
    "- Standardize format (prefer ISO: YYYY-MM-DD)\n",
    "- Watch for timezone issues\n",
    "- Excel date serials (numbers like 44562) need special handling\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 2: Floating-Point Precision\n",
    "\n",
    "**Surprise:** Computers can't represent all decimal numbers exactly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.528444Z",
     "iopub.status.busy": "2025-10-13T14:25:24.528379Z",
     "iopub.status.idle": "2025-10-13T14:25:24.529858Z",
     "shell.execute_reply": "2025-10-13T14:25:24.529677Z"
    }
   },
   "outputs": [],
   "source": [
    "# The famous 0.1 + 0.2 problem\n",
    "result = 0.1 + 0.2\n",
    "print(f\"0.1 + 0.2 = {result}\")\n",
    "print(f\"Does 0.1 + 0.2 == 0.3? {result == 0.3}\")\n",
    "print(f\"\\nWhy? Binary representation limitation.\")\n",
    "print(f\"Actual value: {result:.20f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When This Matters:**\n",
    "- Financial calculations (money)\n",
    "- Equality checks (`==` can fail)\n",
    "- Accumulating small amounts (errors compound)\n",
    "\n",
    "**Solutions:**\n",
    "- Use `Decimal` type for money\n",
    "- Use `np.isclose()` or `round()` for comparisons\n",
    "- Round final results appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.530696Z",
     "iopub.status.busy": "2025-10-13T14:25:24.530638Z",
     "iopub.status.idle": "2025-10-13T14:25:24.532075Z",
     "shell.execute_reply": "2025-10-13T14:25:24.531914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Better: Use np.isclose() for float comparisons\n",
    "print(f\"np.isclose(0.1 + 0.2, 0.3): {np.isclose(0.1 + 0.2, 0.3)}\")\n",
    "\n",
    "# Or round for display\n",
    "print(f\"round(0.1 + 0.2, 2) == 0.3: {round(0.1 + 0.2, 2) == 0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pitfall 3: Numbers Stored as Strings\n",
    "\n",
    "**Very common in real-world data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.532905Z",
     "iopub.status.busy": "2025-10-13T14:25:24.532853Z",
     "iopub.status.idle": "2025-10-13T14:25:24.534823Z",
     "shell.execute_reply": "2025-10-13T14:25:24.534658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numbers with formatting stored as strings\n",
    "messy_numbers = pd.DataFrame({\n",
    "    'product': ['Widget', 'Gadget', 'Doohickey'],\n",
    "    'price': ['$12.50', '$1,234.56', '45.00']\n",
    "})\n",
    "\n",
    "print(\"Messy numbers:\")\n",
    "print(messy_numbers)\n",
    "print(f\"\\nPrice column type: {messy_numbers['price'].dtype}\")\n",
    "print(\"\\nTry to calculate total:\")\n",
    "try:\n",
    "    total = messy_numbers['price'].sum()\n",
    "    print(f\"Total: {total}\")\n",
    "    print(\"Uh oh... that's not right!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.535608Z",
     "iopub.status.busy": "2025-10-13T14:25:24.535551Z",
     "iopub.status.idle": "2025-10-13T14:25:24.537835Z",
     "shell.execute_reply": "2025-10-13T14:25:24.537659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Clean and convert\n",
    "clean_numbers = messy_numbers.copy()\n",
    "\n",
    "# Remove $ and , then convert to float\n",
    "clean_numbers['price'] = clean_numbers['price'].str.replace('$', '', regex=False)\n",
    "clean_numbers['price'] = clean_numbers['price'].str.replace(',', '', regex=False)\n",
    "clean_numbers['price'] = clean_numbers['price'].astype(float)\n",
    "\n",
    "print(\"Clean numbers:\")\n",
    "print(clean_numbers)\n",
    "print(f\"\\nPrice column type: {clean_numbers['price'].dtype}\")\n",
    "print(f\"Total: ${clean_numbers['price'].sum():,.2f}\")\n",
    "print(\"âœ… Now math works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Examples:**\n",
    "- Percentages: \"5%\" â†’ 0.05\n",
    "- Units embedded: \"42.5 kg\" â†’ 42.5\n",
    "- Phone numbers: \"(555) 123-4567\" â†’ Keep as string or extract digits\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 4: Booleans\n",
    "\n",
    "Many ways to represent True/False!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.538691Z",
     "iopub.status.busy": "2025-10-13T14:25:24.538626Z",
     "iopub.status.idle": "2025-10-13T14:25:24.540331Z",
     "shell.execute_reply": "2025-10-13T14:25:24.540147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Many representations of boolean\n",
    "bool_mess = pd.DataFrame({\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'is_active': ['Yes', 'No', 'Y', 'N']\n",
    "})\n",
    "\n",
    "print(\"Messy booleans:\")\n",
    "print(bool_mess)\n",
    "print(f\"\\nis_active type: {bool_mess['is_active'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.541115Z",
     "iopub.status.busy": "2025-10-13T14:25:24.541062Z",
     "iopub.status.idle": "2025-10-13T14:25:24.543094Z",
     "shell.execute_reply": "2025-10-13T14:25:24.542931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Standardize\n",
    "bool_clean = bool_mess.copy()\n",
    "bool_clean['is_active'] = bool_clean['is_active'].map({\n",
    "    'Yes': True,\n",
    "    'Y': True,\n",
    "    'No': False,\n",
    "    'N': False\n",
    "})\n",
    "\n",
    "print(\"Clean booleans:\")\n",
    "print(bool_clean)\n",
    "print(f\"\\nis_active type: {bool_clean['is_active'].dtype}\")\n",
    "print(f\"\\nActive customers: {bool_clean['is_active'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Type Pitfalls\n",
    "\n",
    "1. **Dates**: Parse explicitly, standardize format\n",
    "2. **Floats**: Watch for precision issues, don't use `==`\n",
    "3. **Numbers as strings**: Clean formatting, then convert\n",
    "4. **Booleans**: Standardize representations\n",
    "\n",
    "**Always check types** with `df.dtypes` after loading data!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Missing Values\n",
    "\n",
    "> **\"Missing data IS dataâ€”but different representations mean different things. Your choice matters.\"**\n",
    "\n",
    "### Common Representations of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.543977Z",
     "iopub.status.busy": "2025-10-13T14:25:24.543923Z",
     "iopub.status.idle": "2025-10-13T14:25:24.546133Z",
     "shell.execute_reply": "2025-10-13T14:25:24.545956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Many ways to represent \"missing\"\n",
    "missing_examples = pd.DataFrame({\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, np.nan, 30, -999, 35],\n",
    "    'city': ['NYC', '', 'Boston', 'N/A', 'Chicago'],\n",
    "    'income': [50000, 60000, 0, 70000, np.nan]\n",
    "})\n",
    "\n",
    "print(\"Dataset with various missing value representations:\")\n",
    "print(missing_examples)\n",
    "print(\"\\nWhat's missing here?\")\n",
    "print(\"- Bob's age: NaN (proper missing)\")\n",
    "print(\"- Bob's city: empty string''\")\n",
    "print(\"- Charlie's income: 0 (is this missing or actually zero?)\")\n",
    "print(\"- Diana's age: -999 (sentinel value)\")\n",
    "print(\"- Diana's city: 'N/A' (string sentinel)\")\n",
    "print(\"- Eve's income: NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why It Matters: Different Representations, Different Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.547079Z",
     "iopub.status.busy": "2025-10-13T14:25:24.547030Z",
     "iopub.status.idle": "2025-10-13T14:25:24.548869Z",
     "shell.execute_reply": "2025-10-13T14:25:24.548680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: How NULL vs 0 affects calculations\n",
    "with_null = pd.Series([10, 20, np.nan])\n",
    "with_zero = pd.Series([10, 20, 0])\n",
    "\n",
    "print(\"=== Impact on Aggregations ===\")\n",
    "print(f\"\\nWith NULL: [10, 20, NaN]\")\n",
    "print(f\"  Mean: {with_null.mean():.2f}  (NULLs excluded)\")\n",
    "print(f\"  Count: {with_null.count()}  (counts non-NULL)\")\n",
    "\n",
    "print(f\"\\nWith Zero: [10, 20, 0]\")\n",
    "print(f\"  Mean: {with_zero.mean():.2f}  (zero included!)\")\n",
    "print(f\"  Count: {with_zero.count()}  (counts all)\")\n",
    "\n",
    "print(\"\\nâš ï¸  Same data, but 0 vs NaN gives different results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Framework: What Does \"Missing\" Mean?\n",
    "\n",
    "Before handling missing values, ask:\n",
    "\n",
    "1. **\"Not applicable\"**\n",
    "   - This field doesn't apply to this row\n",
    "   - Example: \"spouse_name\" for single person\n",
    "   - Strategy: Leave as NULL or N/A\n",
    "\n",
    "2. **\"Unknown\"**\n",
    "   - We don't know the value but it exists\n",
    "   - Example: Customer age not collected\n",
    "   - Strategy: NULL or impute (fill with median/mean)\n",
    "\n",
    "3. **\"Not collected yet\"**\n",
    "   - Temporal gap, data will arrive later\n",
    "   - Example: Survey response pending\n",
    "   - Strategy: Mark as pending, don't analyze yet\n",
    "\n",
    "**Your representation choice should reflect the meaning!**\n",
    "\n",
    "### Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.549772Z",
     "iopub.status.busy": "2025-10-13T14:25:24.549720Z",
     "iopub.status.idle": "2025-10-13T14:25:24.551575Z",
     "shell.execute_reply": "2025-10-13T14:25:24.551402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Standardizing missing values\n",
    "messy_missing = pd.DataFrame({\n",
    "    'product': ['Widget', 'Gadget', 'Doohickey', 'Thingamajig'],\n",
    "    'price': [10.0, np.nan, 'N/A', '']\n",
    "})\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(messy_missing)\n",
    "print(f\"\\nMissing values detected: {messy_missing['price'].isna().sum()}\")\n",
    "print(\"Problem: NaN, 'N/A', and '' are all missing, but pandas only sees NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.552493Z",
     "iopub.status.busy": "2025-10-13T14:25:24.552413Z",
     "iopub.status.idle": "2025-10-13T14:25:24.554592Z",
     "shell.execute_reply": "2025-10-13T14:25:24.554403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution: Standardize to NULL (NaN)\n",
    "clean_missing = messy_missing.copy()\n",
    "\n",
    "# Replace common sentinel values with NaN\n",
    "clean_missing['price'] = clean_missing['price'].replace(['N/A', '', 'Unknown', 'ERROR'], np.nan)\n",
    "\n",
    "print(\"After standardization:\")\n",
    "print(clean_missing)\n",
    "print(f\"\\nMissing values detected: {clean_missing['price'].isna().sum()}\")\n",
    "print(\"âœ… Now all missing values are represented consistently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Strategies\n",
    "\n",
    "Once standardized, you have options:\n",
    "\n",
    "1. **Drop rows** with missing values\n",
    "   - `df.dropna(subset=['column'])`\n",
    "   - Use when: Missing data is small percentage\n",
    "   - Risk: Losing information\n",
    "\n",
    "2. **Drop columns** with too many missing values\n",
    "   - `df.dropna(axis=1, thresh=len(df)*0.5)`\n",
    "   - Use when: Column is mostly empty\n",
    "\n",
    "3. **Fill with default value**\n",
    "   - `df['column'].fillna(0)` or `.fillna('Unknown')`\n",
    "   - Use when: There's a meaningful default\n",
    "   - Document your choice!\n",
    "\n",
    "4. **Impute with statistics**\n",
    "   - `df['age'].fillna(df['age'].median())`\n",
    "   - Use when: Missing at random\n",
    "   - Risk: Creating fake data\n",
    "\n",
    "5. **Keep as NULL**\n",
    "   - Leave as NaN\n",
    "   - Use when: Missing is meaningful (not applicable)\n",
    "   - Be aware how it affects aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:25:24.555494Z",
     "iopub.status.busy": "2025-10-13T14:25:24.555427Z",
     "iopub.status.idle": "2025-10-13T14:25:24.559032Z",
     "shell.execute_reply": "2025-10-13T14:25:24.558838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Different strategies\n",
    "data_with_missing = pd.DataFrame({\n",
    "    'transaction_id': ['TXN001', 'TXN002', 'TXN003', 'TXN004'],\n",
    "    'amount': [100, 200, np.nan, 150],\n",
    "    'notes': ['Paid', np.nan, 'Paid', np.nan]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data_with_missing)\n",
    "\n",
    "# Strategy 1: Fill amount with median\n",
    "strategy1 = data_with_missing.copy()\n",
    "strategy1['amount'] = strategy1['amount'].fillna(strategy1['amount'].median())\n",
    "print(\"\\nStrategy 1: Fill amount with median\")\n",
    "print(strategy1)\n",
    "\n",
    "# Strategy 2: Fill notes with 'No notes'\n",
    "strategy2 = data_with_missing.copy()\n",
    "strategy2['notes'] = strategy2['notes'].fillna('No notes')\n",
    "print(\"\\nStrategy 2: Fill notes with 'No notes'\")\n",
    "print(strategy2)\n",
    "\n",
    "# Strategy 3: Drop rows with any missing\n",
    "strategy3 = data_with_missing.dropna()\n",
    "print(\"\\nStrategy 3: Drop rows with ANY missing values\")\n",
    "print(strategy3)\n",
    "print(f\"Rows remaining: {len(strategy3)} out of {len(data_with_missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Your Choices!\n",
    "\n",
    "**Example documentation:**\n",
    "\n",
    "> **Handling missing payment_method values:** Found 2,579 transactions (25.8%) with NULL payment method. These represent transactions where payment method was not recorded at point of sale. \n",
    ">\n",
    "> **Decision:** Convert to NaN and exclude from payment method analysis. Our \"sales by payment method\" report will only include transactions with known payment methods (74.2% of data).\n",
    ">\n",
    "> **Alternative considered:** Could create \"Unknown\" category, but this might mislead stakeholders into thinking it's a valid payment option. \n",
    ">\n",
    "> **Implication:** Total sales across all payment methods will be less than total sales overall. Need to clearly communicate this in reporting.\n",
    "\n",
    "### Summary: Missing Values\n",
    "\n",
    "1. **Standardize** to NULL/NaN first\n",
    "2. **Understand** what \"missing\" means for each column\n",
    "3. **Choose** a strategy (drop, fill, impute, keep)\n",
    "4. **Document** your choice and implications\n",
    "5. **Communicate** how it affects downstream analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### 1. Data Structure is Strategic\n",
    "- You're defining the language the business uses\n",
    "- Databases solve: \"Store each fact once, answer any question\"\n",
    "\n",
    "### 2. Tidy Data Principles\n",
    "- Each variable is a column\n",
    "- Each observation is a row\n",
    "- Each value is a cell\n",
    "\n",
    "### 3. Five Common Problems\n",
    "1. Column headers are values\n",
    "2. Multiple variables in one column\n",
    "3. Variables in rows AND columns\n",
    "4. Multiple observational units in one table\n",
    "5. Single observational unit across multiple tables\n",
    "\n",
    "### 4. Primary Keys\n",
    "- Every dataset needs a unique identifier\n",
    "- Always validate with assertions\n",
    "- Foundation for joins and data integrity\n",
    "\n",
    "### 5. Types Matter\n",
    "- Dates: Parse explicitly\n",
    "- Floats: Watch precision, don't use `==`\n",
    "- Strings-as-numbers: Clean then convert\n",
    "- Booleans: Standardize representations\n",
    "\n",
    "### 6. Missing Values\n",
    "- Different representations mean different things\n",
    "- Standardize to NULL/NaN\n",
    "- Document your handling strategy\n",
    "- Understand impact on aggregations\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**In-Class Exercise:**\n",
    "- Apply these concepts to real messy data\n",
    "- Practice identifying and fixing problems\n",
    "- Build your data cleaning skills\n",
    "\n",
    "**Notebook:** `day1_exercise_tidy.ipynb`  \n",
    "**Dataset:** `data/day1/dirty_cafe_sales.csv`  \n",
    "**Data Dictionary:** `data/day1/README.md`\n",
    "\n",
    "**Let's practice!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
